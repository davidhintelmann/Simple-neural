{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.rcParams[\"figure.figsize\"] = (1,1)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCifar(filename):\n",
    "    with open('data/cifar-10-batches-py/'+filename, 'rb') as file:\n",
    "        data = pickle.load(file, encoding='latin1')\n",
    "    feature = data['data']\n",
    "    label = data['labels']\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1, labels_1 = loadCifar('data_batch_1')\n",
    "batch_2, labels_2 = loadCifar('data_batch_2')\n",
    "batch_3, labels_3 = loadCifar('data_batch_3')\n",
    "batch_4, labels_4 = loadCifar('data_batch_4')\n",
    "batch_5, labels_5 = loadCifar('data_batch_5')\n",
    "batch_test, labels_test = loadCifar('test_batch') \n",
    "\n",
    "features = np.concatenate([batch_1,batch_2,batch_3,batch_4,batch_5],0)\n",
    "labels_train = np.concatenate([labels_1,labels_2,labels_3,labels_4,labels_5],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapeData(batch):\n",
    "    assert batch.shape[1] == 3072\n",
    "    r = batch[:, 0:1024].reshape(batch.shape[0], 1, 32, 32)\n",
    "    g = batch[:, 1024:2048].reshape(batch.shape[0], 1, 32, 32)\n",
    "    b = batch[:, 2048:3072].reshape(batch.shape[0], 1, 32, 32)\n",
    "    rgb = np.concatenate([r,g,b], axis=1)\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3, 32, 32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape #fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = shapeData(features)\n",
    "features_test = shapeData(batch_test)\n",
    "#labels_test = np.array(labels_test)\n",
    "\n",
    "#labels_train = torch.from_numpy(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "a = np.transpose(features_train[12],(1,2,0))\n",
    "plt.imshow(a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFARarray(Dataset): \n",
    "    def __init__(self, data, label, transform=None):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.img_shape = data.shape\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.fromarray(np.transpose(self.data[index]))\n",
    "        label = self.label[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img_to_tensor = transforms.ToTensor()\n",
    "            img = img_to_tensor(img)\n",
    "        return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def plot_image(self, number):\n",
    "        file = self.data\n",
    "        label = self.label\n",
    "        fig = plt.figure(figsize = (3,2))\n",
    "        plt.imshow(file[number])\n",
    "        plt.title(classes[label[number]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "\n",
    "#features_train = torch.Tensor(features_train)\n",
    "#labels_train = torch.Tensor(labels_train)\n",
    "trainload = CIFARarray(features_train,labels_train.tolist(),transform_train)\n",
    "trainloade = torch.utils.data.DataLoader(trainload, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "testload = CIFARarray(features_test,labels_test,transform_train)\n",
    "testloade = torch.utils.data.DataLoader(testload, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc = nn.Linear(in_features=16 * 16 * 24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.relu1(output)\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.pool(output)\n",
    "        output = self.conv3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.conv4(output)\n",
    "        output = self.relu4(output)\n",
    "        output = output.view(-1, 16 * 16 * 24)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netty = SimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "a = Image.fromarray(np.transpose(features_train[12],(1,2,0)))\n",
    "#plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = iter(tmp).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.246\n",
      "[1,  4000] loss: 1.206\n",
      "[1,  6000] loss: 1.199\n",
      "[1,  8000] loss: 1.208\n",
      "[1, 10000] loss: 1.191\n",
      "[1, 12000] loss: 1.201\n",
      "[2,  2000] loss: 1.111\n",
      "[2,  4000] loss: 1.129\n",
      "[2,  6000] loss: 1.118\n",
      "[2,  8000] loss: 1.126\n",
      "[2, 10000] loss: 1.117\n",
      "[2, 12000] loss: 1.122\n",
      "[3,  2000] loss: 1.032\n",
      "[3,  4000] loss: 1.063\n",
      "[3,  6000] loss: 1.059\n",
      "[3,  8000] loss: 1.042\n",
      "[3, 10000] loss: 1.058\n",
      "[3, 12000] loss: 1.071\n",
      "[4,  2000] loss: 0.985\n",
      "[4,  4000] loss: 0.990\n",
      "[4,  6000] loss: 1.010\n",
      "[4,  8000] loss: 1.017\n",
      "[4, 10000] loss: 1.006\n",
      "[4, 12000] loss: 1.002\n",
      "[5,  2000] loss: 0.922\n",
      "[5,  4000] loss: 0.971\n",
      "[5,  6000] loss: 0.943\n",
      "[5,  8000] loss: 0.960\n",
      "[5, 10000] loss: 0.969\n",
      "[5, 12000] loss: 0.967\n",
      "[6,  2000] loss: 0.888\n",
      "[6,  4000] loss: 0.896\n",
      "[6,  6000] loss: 0.926\n",
      "[6,  8000] loss: 0.916\n",
      "[6, 10000] loss: 0.965\n",
      "[6, 12000] loss: 0.938\n",
      "[7,  2000] loss: 0.846\n",
      "[7,  4000] loss: 0.852\n",
      "[7,  6000] loss: 0.888\n",
      "[7,  8000] loss: 0.898\n",
      "[7, 10000] loss: 0.905\n",
      "[7, 12000] loss: 0.939\n",
      "[8,  2000] loss: 0.803\n",
      "[8,  4000] loss: 0.839\n",
      "[8,  6000] loss: 0.869\n",
      "[8,  8000] loss: 0.879\n",
      "[8, 10000] loss: 0.886\n",
      "[8, 12000] loss: 0.889\n",
      "[9,  2000] loss: 0.773\n",
      "[9,  4000] loss: 0.815\n",
      "[9,  6000] loss: 0.822\n",
      "[9,  8000] loss: 0.840\n",
      "[9, 10000] loss: 0.876\n",
      "[9, 12000] loss: 0.851\n",
      "[10,  2000] loss: 0.756\n",
      "[10,  4000] loss: 0.807\n",
      "[10,  6000] loss: 0.801\n",
      "[10,  8000] loss: 0.836\n",
      "[10, 10000] loss: 0.818\n",
      "[10, 12000] loss: 0.842\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloade, 0):\n",
    "        img, label = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(img)\n",
    "        #print(outputs.type())\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 60 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloade:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 54 %\n",
      "Accuracy of   car : 75 %\n",
      "Accuracy of  bird : 44 %\n",
      "Accuracy of   cat : 38 %\n",
      "Accuracy of  deer : 62 %\n",
      "Accuracy of   dog : 56 %\n",
      "Accuracy of  frog : 63 %\n",
      "Accuracy of horse : 64 %\n",
      "Accuracy of  ship : 74 %\n",
      "Accuracy of truck : 69 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloade:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy with Epoch 2 & simple network:\n",
    "\n",
    "Accuracy of the network on the 10000 test images: 53 %\n",
    "\n",
    "Accuracy of plane : 47 %  \n",
    "Accuracy of   car : 80 %  \n",
    "Accuracy of  bird : 37 %  \n",
    "Accuracy of   cat : 35 %  \n",
    "Accuracy of  deer : 53 %  \n",
    "Accuracy of   dog : 55 %  \n",
    "Accuracy of  frog : 48 %  \n",
    "Accuracy of horse : 55 %  \n",
    "Accuracy of  ship : 77 %  \n",
    "Accuracy of truck : 41 %  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy with Epoch 10 & simple network:\n",
    "\n",
    "Accuracy of the network on the 10000 test images: 60 %  \n",
    "\n",
    "Accuracy of plane : 54 %  \n",
    "Accuracy of   car : 75 %  \n",
    "Accuracy of  bird : 44 %  \n",
    "Accuracy of   cat : 38 %  \n",
    "Accuracy of  deer : 62 %  \n",
    "Accuracy of   dog : 56 %  \n",
    "Accuracy of  frog : 63 %  \n",
    "Accuracy of horse : 64 %  \n",
    "Accuracy of  ship : 74 %  \n",
    "Accuracy of truck : 69 %  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py382] *",
   "language": "python",
   "name": "conda-env-py382-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
